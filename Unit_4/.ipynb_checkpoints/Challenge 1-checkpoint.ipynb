{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import spacy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function:\n",
    "\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and applying cleaning function:\n",
    "\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "persuasion = re.sub(r'Chapter /d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping the texts to work in spaCy:\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Dataframe of the sentences in each text:\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bag_of_ads(text):\n",
    "    \n",
    "    ads2 = [token.text\n",
    "           for token in text\n",
    "           if token.pos_ == 'ADJ'\n",
    "           if not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(ads2).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boa_features(sentences, common_words):\n",
    "    \n",
    "        # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to ads, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.text\n",
    "                 for token in sentence\n",
    "                 if (not token.is_stop\n",
    "                     and token.text in common_words)]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n"
     ]
    }
   ],
   "source": [
    "# Getting the most common adjectives for each text:\n",
    "alice_ads = bag_of_ads(alice_doc)\n",
    "persuasion_ads = bag_of_ads(persuasion_doc)\n",
    "\n",
    "# Combining theit counts to avoid duplicates:\n",
    "alice_persuasion_ads = set(alice_ads + persuasion_ads)\n",
    "\n",
    "# Getting the count per sentence and making a dataframe:\n",
    "ads_df = boa_features(sentences, alice_persuasion_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc2 = ensemble.RandomForestClassifier()\n",
    "\n",
    "Y = ads_df['text_source']\n",
    "X = ads_df.drop(['text_sentence', 'text_source'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size = .5,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = rfc2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256479299899024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now trying to see if adding adverbs would help the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bag_of_adverbs(text):\n",
    "    \n",
    "    ads2 = [token.text\n",
    "           for token in text\n",
    "           if token.pos_ == 'ADV'\n",
    "           if not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(ads2).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_adverbs_features(sentences, common_words):\n",
    "    \n",
    "        # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to ads, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.text\n",
    "                 for token in sentence\n",
    "                 if (not token.is_stop\n",
    "                     and token.text in common_words)]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n"
     ]
    }
   ],
   "source": [
    "# Getting the most common adjectives for each text:\n",
    "alice_adverbs = bag_of_adverbs(alice_doc)\n",
    "persuasion_adverbs = bag_of_adverbs(persuasion_doc)\n",
    "\n",
    "# Combining their counts to avoid duplicates:\n",
    "alice_persuasion_adverbs = set(alice_adverbs + persuasion_adverbs)\n",
    "\n",
    "# Getting the count per sentence and making a dataframe:\n",
    "adverbs_df = bo_adverbs_features(sentences, alice_persuasion_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the adverb and adjective dfs:\n",
    "ads_df2 = ads_df.drop(['text_source', 'text_sentence'], axis = 1)\n",
    "combined_df = pd.concat([ads_df2, adverbs_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = combined_df.drop(['text_sentence', 'text_source'], axis = 1)\n",
    "Y2 = combined_df['text_source']\n",
    "\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, \n",
    "                                                        test_size = .4,\n",
    "                                                        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8429172510518934"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc3 = ensemble.RandomForestClassifier()\n",
    "rfc3.fit(X2_train, Y2_train)\n",
    "\n",
    "rfc3.score(X2_train, Y2_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7346509671993272"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc3.score(X2_test, Y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a different text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensibility = gutenberg.raw('austen-sense.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[\\[].*?[\\]]\"\n",
    "\n",
    "sensibility = re.sub(pattern, \"\", sensibility)\n",
    "sensibility = re.sub(r'CHAPTER \\d+', '', sensibility)\n",
    "sensibility = ' '.join(sensibility.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The family of Dashwood had long been settled in Sussex. Their estate was large, and their residence '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensibility[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensibility_doc = nlp(sensibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensibility_sents = [[sents, 'Austen'] for sents in sensibility_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common adverbs and adjectives:\n",
    "\n",
    "sensibility_ads = bag_of_ads(sensibility_doc)\n",
    "sensibility_adverbs = bag_of_adverbs(sensibility_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ads = set(alice_ads + persuasion_ads + sensibility_ads)\n",
    "all_adverbs = set(alice_adverbs + persuasion_adverbs + sensibility_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-e4a13f6e80c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mads_df3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboa_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensibility_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madverbs_df3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbo_adverbs_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensibility_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_adverbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-e08f4c9f70a3>\u001b[0m in \u001b[0;36mboa_features\u001b[0;34m(sentences, common_words)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# stop words, and uncommon words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         words = [token.text\n\u001b[0;32m---> 15\u001b[0;31m                  \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                  if (not token.is_stop\n\u001b[1;32m     17\u001b[0m                      and token.text in common_words)]\n",
      "\u001b[0;32m<ipython-input-74-e08f4c9f70a3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m         words = [token.text\n\u001b[1;32m     15\u001b[0m                  \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                  if (not token.is_stop\n\u001b[0m\u001b[1;32m     17\u001b[0m                      and token.text in common_words)]\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_stop'"
     ]
    }
   ],
   "source": [
    "ads_df3 = boa_features(sensibility_sents, all_ads)\n",
    "adverbs_df3 = bo_adverbs_features(sensibility_sents, all_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
