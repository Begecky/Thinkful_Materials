{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator as op\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing, Cleaning and Selecting Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Epicurious.csv')\n",
    "data2 = data.drop(['calories', 'protein', 'fat', 'sodium'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Nutrtional Info:\n",
    "\n",
    "rating_corr = data.corr().rating\n",
    "rating_corr = rating_corr.sort_values(ascending = False)\n",
    "\n",
    "key_list = list(rating_corr[0:50].keys())\n",
    "data = pd.DataFrame(data[key_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Nutrtional Info:\n",
    "\n",
    "rating_corr2 = data2.corr().rating\n",
    "rating_corr2 = rating_corr2.sort_values(ascending = False)\n",
    "\n",
    "key_list2 = list(rating_corr2[0:31].keys())\n",
    "data2 = pd.DataFrame(data2[key_list2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification Party:\n",
    "data.loc[data['rating'] >= 4, 'binary_rating'] = 1\n",
    "data.loc[data['rating'] < 4, 'binary_rating'] = 0\n",
    "\n",
    "data2.loc[data2['rating'] >= 4, 'binary_rating'] = 1\n",
    "data2.loc[data2['rating'] < 4, 'binary_rating'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting our Training and Test Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X1 = data2.drop(['rating', 'binary_rating'], 1)\n",
    "y1 = data2.binary_rating\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Model (Small Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_1 = SVC(kernel = 'linear')\n",
    "svc_1.fit(X1_train, y1_train)\n",
    "\n",
    "y1_pred_ = svc_1.predict(X1_test)\n",
    "\n",
    "svc_1_cfmat = confusion_matrix(y1_test, y1_pred_, labels = [0, 1])\n",
    "svc_1_cvscores = cross_val_score(svc_1, X1, y1, cv = 10)\n",
    "svc_1_trainscore = svc_1.score(X1_train, y1_train)\n",
    "svc_1_testscore = svc_1.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are: \n",
      " [[ 1.99860836e+00  8.59749428e-04  2.05594448e-04 -3.20998315e-04\n",
      "   1.58906287e-04  1.02660349e-03  9.66509176e-05 -1.07255392e-04\n",
      "   5.16524306e-04 -5.87026161e-04 -2.20392046e-04  5.77719992e-04\n",
      "   5.21931170e-04  3.39467966e-04  1.05890835e-03 -9.67217955e-05\n",
      "   1.99758571e+00 -4.42523820e-04  3.04112173e-04 -2.86772277e-04\n",
      "  -1.16023226e-04  6.66492278e-04  6.09964607e-04  2.02119523e-04\n",
      "   7.06738111e-04  4.46270631e-04  6.64260310e-04 -1.81036898e-04\n",
      "   5.75005480e-04  1.29589169e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The coefficients are: \\n {}\".format(svc_1.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV Scores are: \n",
      " [0.55284148 0.55383848 0.55732802 0.56530409 0.54912718 0.55062344\n",
      " 0.57406484 0.53516209 0.55239521 0.55339321]\n"
     ]
    }
   ],
   "source": [
    "print(\"The CV Scores are: \\n {}\".format(svc_1_cvscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean and STD of the CV Scores are:\n",
      " Mean:0.5544078043071993. STD: 0.009664971552262066.\n",
      "The Training score is: 0.5536437877937784\n",
      "The Test score is: 0.5574669658439292\n",
      "[[ 454 1401]\n",
      " [ 374 1782]]\n",
      "Specificity:[[0.24474394]]\n",
      "Sensitivity:[[0.82653061]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Mean and STD of the CV Scores are:\\n Mean:{}. STD: {}.\".format(svc_1_cvscores.mean(), \n",
    "                                                                          svc_1_cvscores.std()))\n",
    "print(\"The Training score is: {}\".format(svc_1_trainscore))\n",
    "print(\"The Test score is: {}\".format(svc_1_testscore))\n",
    "print(svc_1_cfmat)\n",
    "print(\"Specificity:{}\".format(svc_1_cfmat[0:1, 0:1]/(svc_1_cfmat[0:1, 0:1] + svc_1_cfmat[0:1, 1:2])))\n",
    "print(\"Sensitivity:{}\".format(svc_1_cfmat[1:2, 1:2]/(svc_1_cfmat[1:2, 0:1] + svc_1_cfmat[1:2, 1:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts for Data2: 4.375    8019\n",
      "3.750    5169\n",
      "5.000    2719\n",
      "0.000    1836\n",
      "3.125    1489\n",
      "2.500     532\n",
      "1.250     164\n",
      "1.875     124\n",
      "Name: rating, dtype: int64\n",
      "Value Counts for our Binary Column: \n",
      " 1.0    10738\n",
      "0.0     9314\n",
      "Name: binary_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Counts for Data2: {}\".format(data2.rating.value_counts()))\n",
    "print(\"Value Counts for our Binary Column: \\n {}\".format(data2.binary_rating.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given that our binary classes are diveded fairly evenly, we don't have to worry about class imbalance here - our main concern is the lack of correlation amongst the features and the outcome variable.  Using a binary outcome variable has helped, and it would perhaps help as well to lower our cut-off to 3.75 instead of 4, which we'll do below, after investigated our larger data frame ('data') which includes the nutrtional information and a larger amount of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Mode (Large Datasetl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data.drop(['rating', 'binary_rating'], 1)\n",
    "y2 = data.binary_rating\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_2 = SVC(kernel = 'linear')\n",
    "svc_2.fit(X2_train, y2_train)\n",
    "\n",
    "y2_pred_ = svc_2.predict(X2_test)\n",
    "\n",
    "svc_2_cfmat = confusion_matrix(y2_test, y2_pred_, labels = [0, 1])\n",
    "svc_2_cvscores = cross_val_score(svc_2, X2, y2, cv = 10)\n",
    "svc_2_trainscore = svc_2.score(X2_train, y2_train)\n",
    "svc_2_testscore = svc_2.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are: \n",
      " [[ 1.24847418  0.15716924  0.24507018 -0.04072991  0.15790374  0.43825547\n",
      "   0.07248807 -0.02526013  0.23835764 -0.07920606 -0.07817526  0.28581627\n",
      "   0.34238851  0.20884113  0.43808281  0.08278367  0.76124946 -0.2090276\n",
      "   0.20526987 -0.12059186  0.04838711  0.09694742  0.24045194  0.11577093\n",
      "   0.39982624  0.19171237  0.39590641 -0.08369852  0.47475643  0.11344484\n",
      "   0.6539698   0.28893508  0.04175122  0.21631996  0.3363189   0.19718094\n",
      "   0.33130265  0.31289277  0.33595707  0.06784213 -0.36401064  0.33032709\n",
      "   0.34855891  0.23676866  0.31151587  0.09601881  0.36725806  0.17222813\n",
      "  -0.0092481 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The coefficients are: \\n {}\".format(svc_2.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV Scores are: \n",
      " [0.56779661 0.5667996  0.55633101 0.58624128 0.56608479 0.57605985\n",
      " 0.59351621 0.5446384  0.57235529 0.55289421]\n"
     ]
    }
   ],
   "source": [
    "print(\"The CV Scores are: \\n {}\".format(svc_2_cvscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean and STD of the CV Scores are:\n",
      " Mean:0.5682717247384779. STD: 0.014079336862241885.\n",
      "The Training score is: 0.5760239386571909\n",
      "The Test score is: 0.5676888556469708\n",
      "[[ 773 1073]\n",
      " [ 661 1504]]\n",
      "Specificity:[[0.41874323]]\n",
      "Sensitivity:[[0.69468822]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Mean and STD of the CV Scores are:\\n Mean:{}. STD: {}.\".format(svc_2_cvscores.mean(), \n",
    "                                                                          svc_2_cvscores.std()))\n",
    "print(\"The Training score is: {}\".format(svc_2_trainscore))\n",
    "print(\"The Test score is: {}\".format(svc_2_testscore))\n",
    "print(svc_2_cfmat)\n",
    "print(\"Specificity:{}\".format(svc_2_cfmat[0:1, 0:1]/(svc_2_cfmat[0:1, 0:1] + svc_2_cfmat[0:1, 1:2])))\n",
    "print(\"Sensitivity:{}\".format(svc_2_cfmat[1:2, 1:2]/(svc_2_cfmat[1:2, 0:1] + svc_2_cfmat[1:2, 1:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts for Data: 4.375    8019\n",
      "3.750    5169\n",
      "5.000    2719\n",
      "0.000    1836\n",
      "3.125    1489\n",
      "2.500     532\n",
      "1.250     164\n",
      "1.875     124\n",
      "Name: rating, dtype: int64\n",
      "Value Counts for our Binary Column: \n",
      " 1.0    10738\n",
      "0.0     9314\n",
      "Name: binary_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Counts for Data: {}\".format(data.rating.value_counts()))\n",
    "print(\"Value Counts for our Binary Column: \\n {}\".format(data.binary_rating.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This second model has a higher training score and a lower test score (the inverse of the model above).  Preferrably, we would get a better test score with the training score a bit lower.  The mean cv score on the second model is also higher while more standard deviation in comparison with the first. You would probably want to go with the model that has laest standard deviation between these folds as you would have a better idea of how it performs on new data input. I would imagine that the second model is somewhat more prone to overfitting since it classifies the positives a bit more accurately and would therefore probably over-classify new input data - especially since this type of data is pretty sparse. \n",
    "\n",
    "> Regarding bias, it is definitely present in this dataset given people who have choosen these recipes are more than likely to give them a rating of four or five. People tend not to choose recipes that they believe they will not like.  Given that they have most-likely choosen a recipe which seemed good in the first place, there is a distinct head-start towards a positive rating.  This would explain why the single elements of food are not necessarily the greatest indicator of a positive rating, but the fact that the recipe was selected in the first place is significantly more indicative. The value counts for the binary rating column demonstrate this: around 2600 datapoints are rated below 3.125 while the remaining 5600 are 3.125 or above.\n",
    "\n",
    "> A way to avoid such a bias would be to distribute the recipes, and especially the foods, to people who haven't necessarily choosen them on the internet. That way, people would be rating a recipe that they had not choosen themselves, but one place in front of them (relatively) randomly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third Model (Small Dataest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['rating'] >= 3.5, 'binary_rating'] = 1\n",
    "data.loc[data['rating'] < 3.5, 'binary_rating'] = 0\n",
    "\n",
    "data2.loc[data2['rating'] >= 3.5, 'binary_rating'] = 1\n",
    "data2.loc[data2['rating'] < 3.5, 'binary_rating'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = data2.drop(['rating', 'binary_rating'], 1)\n",
    "y3 = data2.binary_rating\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_3 = SVC(kernel = 'linear')\n",
    "svc_3.fit(X3_train, y3_train)\n",
    "\n",
    "y3_pred_ = svc_3.predict(X3_test)\n",
    "\n",
    "svc_3_cfmat = confusion_matrix(y3_test, y3_pred_, labels = [0, 1])\n",
    "svc_3_cvscores = cross_val_score(svc_3, X3, y3, cv = 10)\n",
    "svc_3_trainscore = svc_3.score(X3_train, y3_train)\n",
    "svc_3_testscore = svc_3.score(X3_test, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are: \n",
      " [[-2.04935124e-05  9.75497741e-05 -2.58093008e-05 -6.86485562e-06\n",
      "  -8.38012996e-05 -6.27144819e-05 -6.18147127e-05 -7.92615103e-05\n",
      "  -3.06446228e-06  6.46776675e-05  6.01074262e-06 -4.08997081e-05\n",
      "  -1.53747742e-04  2.86105675e-05 -1.04992195e-04 -2.21949164e-07\n",
      "  -1.93808427e-05  1.13841233e-04 -9.72464017e-05 -7.21174716e-05\n",
      "  -5.78212052e-05 -8.56428173e-05 -5.82905961e-05 -4.84404828e-05\n",
      "  -7.70382015e-05  0.00000000e+00 -5.05779041e-05 -3.11551321e-05\n",
      "  -1.60520607e-04  9.32241948e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The coefficients are: \\n {}\".format(svc_3.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV Scores are: \n",
      " [0.79312064 0.79312064 0.79312064 0.79312064 0.79312064 0.79351621\n",
      " 0.79351621 0.79341317 0.79341317 0.79341317]\n"
     ]
    }
   ],
   "source": [
    "print(\"The CV Scores are: \\n {}\".format(svc_3_cvscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean and STD of the CV Scores are:\n",
      " Mean:0.7932875130339416. STD: 0.00017064938924625704.\n",
      "The Training score is: 0.7932173804625646\n",
      "The Test score is: 0.793567688855647\n",
      "[[   0  828]\n",
      " [   0 3183]]\n",
      "Specificity:[[0.]]\n",
      "Sensitivity:[[1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Mean and STD of the CV Scores are:\\n Mean:{}. STD: {}.\".format(svc_3_cvscores.mean(), \n",
    "                                                                          svc_3_cvscores.std()))\n",
    "print(\"The Training score is: {}\".format(svc_3_trainscore))\n",
    "print(\"The Test score is: {}\".format(svc_3_testscore))\n",
    "print(svc_3_cfmat)\n",
    "print(\"Specificity:{}\".format(svc_3_cfmat[0:1, 0:1]/(svc_3_cfmat[0:1, 0:1] + svc_3_cfmat[0:1, 1:2])))\n",
    "print(\"Sensitivity:{}\".format(svc_3_cfmat[1:2, 1:2]/(svc_3_cfmat[1:2, 0:1] + svc_3_cfmat[1:2, 1:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts for our Binary Column: \n",
      " 1.0    15907\n",
      "0.0     4145\n",
      "Name: binary_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Counts for our Binary Column: \\n {}\".format(data2.binary_rating.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite overfitting and you can see from the confusion matrix that it accurately predicts all of the positives but cannot predict a single negative.  The models above are better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: Large Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4= data.drop(['rating', 'binary_rating'], 1)\n",
    "y4= data.binary_rating\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_4 = SVC(kernel = 'linear')\n",
    "svc_4.fit(X4_train, y4_train)\n",
    "\n",
    "y4_pred_ = svc_4.predict(X4_test)\n",
    "\n",
    "svc_4_cfmat = confusion_matrix(y4_test, y4_pred_, labels = [0, 1])\n",
    "svc_4_cvscores = cross_val_score(svc_4, X4, y4, cv = 10)\n",
    "svc_4_trainscore = svc_4.score(X4_train, y4_train)\n",
    "svc_4_testscore = svc_4.score(X4_test, y4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are: \n",
      " [[-1.94446192e-05  2.48987730e-05  7.61503003e-05 -4.12478901e-05\n",
      "  -3.03689424e-05  1.37543224e-05 -1.85004142e-05 -3.37188868e-05\n",
      "  -5.03534927e-05  8.82173318e-05 -5.40017646e-06 -1.35280422e-04\n",
      "  -2.17738830e-04  1.82102590e-05  4.53181033e-06 -5.17545988e-05\n",
      "   2.57162075e-05  3.60692191e-06 -4.18469303e-05 -5.21892699e-05\n",
      "  -1.99512718e-04 -1.48084041e-04 -1.77163532e-05 -9.68824548e-05\n",
      "   7.46144469e-05 -9.51424175e-05  1.85763834e-06  7.29908088e-05\n",
      "  -4.62212743e-05 -1.55326413e-05 -5.57243860e-06 -2.39195940e-06\n",
      "   3.15921653e-06 -2.80248264e-05 -4.73412330e-05 -8.41629069e-05\n",
      "  -4.24725257e-05 -2.18802853e-04  2.72503526e-04 -9.11411913e-05\n",
      "  -1.27997084e-04 -5.51753333e-05 -1.53215097e-04  6.19080035e-05\n",
      "  -3.39788614e-04  3.12158791e-05 -2.29209481e-04  3.07804763e-05\n",
      "  -1.42308639e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The coefficients are: \\n {}\".format(svc_4.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV Scores are: \n",
      " [0.79312064 0.79312064 0.79312064 0.79312064 0.79312064 0.79351621\n",
      " 0.79351621 0.79341317 0.79341317 0.79341317]\n"
     ]
    }
   ],
   "source": [
    "print(\"The CV Scores are: \\n {}\".format(svc_4_cvscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean and STD of the CV Scores are:\n",
      " Mean:0.7932875130339416. STD: 0.00017064938924625704.\n",
      "The Training score is: 0.7913471728695218\n",
      "The Test score is: 0.8010471204188482\n",
      "[[   0  798]\n",
      " [   0 3213]]\n",
      "Specificity:[[0.]]\n",
      "Sensitivity:[[1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Mean and STD of the CV Scores are:\\n Mean:{}. STD: {}.\".format(svc_4_cvscores.mean(), \n",
    "                                                                          svc_4_cvscores.std()))\n",
    "print(\"The Training score is: {}\".format(svc_4_trainscore))\n",
    "print(\"The Test score is: {}\".format(svc_4_testscore))\n",
    "print(svc_4_cfmat)\n",
    "print(\"Specificity:{}\".format(svc_4_cfmat[0:1, 0:1]/(svc_4_cfmat[0:1, 0:1] + svc_4_cfmat[0:1, 1:2])))\n",
    "print(\"Sensitivity:{}\".format(svc_4_cfmat[1:2, 1:2]/(svc_4_cfmat[1:2, 0:1] + svc_4_cfmat[1:2, 1:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts for Data: 4.375    8019\n",
      "3.750    5169\n",
      "5.000    2719\n",
      "0.000    1836\n",
      "3.125    1489\n",
      "2.500     532\n",
      "1.250     164\n",
      "1.875     124\n",
      "Name: rating, dtype: int64\n",
      "Value Counts for our Binary Column: \n",
      " 1.0    15907\n",
      "0.0     4145\n",
      "Name: binary_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Counts for Data: {}\".format(data.rating.value_counts()))\n",
    "print(\"Value Counts for our Binary Column: \\n {}\".format(data.binary_rating.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same here - too much overfitting and not as good as the models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
