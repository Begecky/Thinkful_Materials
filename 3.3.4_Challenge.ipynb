{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colorado Crime Data: Logistic, Lasso and Ridge Regressions:\n",
    "\n",
    "> This dataset is from the same FBI site where the New York Crime data was taken from and represents the same data from Colorado.  Below, we investigate how Logistic, Lasso and Ridge Regressions work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['Population', 'Violent_Crime', 'Murder', 'Rape', 'Rape_2', 'Robbery', \n",
    "            'Assault', 'Property_Crime', 'Burglary', 'Larceny', 'MV_Theft', 'Arson']\n",
    "\n",
    "data = pd.read_excel('Colorado_2013_Crime.xls', names = col_list, header = 3, \n",
    "                     index_col = 0, skiprows = [0], skipfooter = 2).drop('Rape_2', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of feature engineering:\n",
    "\n",
    "# Making a Binary Violent Crime Column:\n",
    "data.loc[data['Violent_Crime'] == 0, 'Binary_Violent_Crime'] = 0\n",
    "data.loc[data['Violent_Crime'] > 0, 'Binary_Violent_Crime'] = 1\n",
    "\n",
    "#Same for Murder and Larceny:\n",
    "data.loc[data['Larceny'] == 0, 'Binary_Larceny'] = 0\n",
    "data.loc[data['Larceny'] > 0, 'Binary_Larceny'] = 1\n",
    "\n",
    "data.loc[data['Murder'] == 0, 'Binary_Murder'] = 0\n",
    "data.loc[data['Murder'] > 0, 'Binary_Murder'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing some per-capita and per-Larceny columns in a new dataframe:\n",
    "\n",
    "data_modified = pd.DataFrame(data)\n",
    "\n",
    "data_modified['Larceny/Capita'] = data_modified.Larceny / data_modified.Population\n",
    "data_modified['Assault/Capita'] = data_modified.Assault / data_modified.Population\n",
    "data_modified['Violent_Crime/Capita'] = data_modified.Violent_Crime / data_modified.Population\n",
    "data_modified['Murder/Capita'] = data_modified.Murder / data_modified.Population\n",
    "\n",
    "data_modified['Violent_Crime/Larceny'] = data_modified.Violent_Crime / data_modified.Larceny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NaN and Inf values:\n",
    "data.loc[data['Violent_Crime/Larceny'] == np.inf] = np.nan\n",
    "data = data.dropna(axis = 0, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_modified.loc[data_modified['Violent_Crime/Larceny'] == np.inf] = np.nan\n",
    "data_modified = data_modified.dropna(axis = 0, how = 'any')\n",
    "\n",
    "# Double checking for cleanliness:\n",
    "# data_modified.where(cond = data_modified.values == np.nan).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables and Models:\n",
    "\n",
    "> After doing a bit of feature engineering, now we can start by defining our input data and our target variables - as well as defining and fitting our first models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our input and target variables (or at least one of them for now) as well as our train/test sets:\n",
    "\n",
    "x = data.drop('Binary_Violent_Crime', 1)\n",
    "\n",
    "y = data.Binary_Violent_Crime\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_regr1 = linear_model.LogisticRegression()\n",
    "\n",
    "log_regr1.fit(X_train, y_train)\n",
    "log_regr1.fit(X_test, y_test)\n",
    "\n",
    "log1_train_score = log_regr1.score(X_train, y_train)\n",
    "log1_test_score = log_regr1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having trouble plotting this:\n",
    "\n",
    "sns.regplot(logistic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score is: 0.9603960396039604\n",
      "The test score is: 0.9615384615384616\n",
      "The coefficients are:\n",
      "[[ 1.96607564e-04  4.81992893e-01  9.54810693e-03  9.25295584e-02\n",
      "   8.02894512e-02  2.99625777e-01  5.67619266e-02 -1.29456571e-01\n",
      "  -7.08045793e-02  2.57023077e-01  5.26381408e-03 -5.88603566e-02\n",
      "   9.54404438e-03 -4.35454438e-04  1.28442031e-04  2.53640385e-04\n",
      "   1.79449678e-06  3.06817878e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Super over-fitting so far:\n",
    "\n",
    "print(\"The training score is: {}\".format(log1_train_score))\n",
    "print(\"The test score is: {}\".format(log1_test_score))\n",
    "print(\"The coefficients are:\\n{}\".format(log_regr1.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients above are pretty large - trying to get them a bit smaller:\n",
    "\n",
    "ridge_regr1 = linear_model.LogisticRegression(penalty = 'l2')\n",
    "\n",
    "ridge_regr1.fit(X_train, y_train)\n",
    "ridge_regr1.fit(X_test, y_test)\n",
    "\n",
    "ridge1_train_score = ridge_regr1.score(X_train, y_train)\n",
    "ridge1_test_score = ridge_regr1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score is: 0.9603960396039604\n",
      "The test score is: 0.9615384615384616\n",
      "The coefficients are:\n",
      "[[ 1.96607565e-04  4.81992894e-01  9.54810696e-03  9.25295587e-02\n",
      "   8.02894514e-02  2.99625777e-01  5.67619266e-02 -1.29456571e-01\n",
      "  -7.08045795e-02  2.57023077e-01  5.26381409e-03 -5.88603568e-02\n",
      "   9.54404442e-03 -4.35454439e-04  1.28442031e-04  2.53640386e-04\n",
      "   1.79449679e-06  3.06817878e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The training score is: {}\".format(ridge1_train_score))\n",
    "print(\"The test score is: {}\".format(ridge1_test_score))\n",
    "print(\"The coefficients are:\\n{}\".format(ridge_regr1.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Coefficients for the first two models are the same - seeing if Lasso/L1 gets us anything different:\n",
    "\n",
    "lasso_regr1 = linear_model.LogisticRegression(penalty = 'l1', max_iter = 400)\n",
    "\n",
    "lasso_regr1.fit(X_train, y_train)\n",
    "lasso_regr1.fit(X_test, y_test)\n",
    "\n",
    "lasso1_train_score = lasso_regr1.score(X_train, y_train)\n",
    "lasso1_test_score = lasso_regr1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score is: 0.9306930693069307\n",
      "The test score is: 0.8461538461538461\n",
      "The coefficients are:\n",
      "[[ 0.00027429  0.07360446  0.          0.          0.04515929  0.07867417\n",
      "   0.00129802 -0.01071891 -0.00483718  0.02492427  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# This looks much better:\n",
    "\n",
    "print(\"The training score is: {}\".format(lasso1_train_score))\n",
    "print(\"The test score is: {}\".format(lasso1_test_score))\n",
    "print(\"The coefficients are:\\n{}\".format(lasso_regr1.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
